Few selected papers from NeuroIPS2019 conference based on my interests.
I will update the list with breif deatils for each paper based on my reading progress.


[] Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
[] Zero-shot Learning via Simultaneous Generating and Learning
[] TensorPipe: Easy Scaling with Micro-Batch Pipeline Parallelism
[] Meta-Learning with Implicit Gradients
[] Deep ReLU Networks Have Surprisingly Few Activation Patterns
[] Zero-Shot Semantic Segmentation
[] NAT: Neural Architecture Transformer for Accurate and Compact Architectures
[] Deep Learning without Weight Transport
[] Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence
[] Neuron Communication Networks
[] Fine-grained Optimization of Deep Neural Networks
[] Multi-View Reinforcement Learning
[] Efficient Meta Learning via Minibatch Proximal Update
[] Positional Normalization
[] Self-Supervised Generalisation with Meta Auxiliary Learning
[] Meta-Learning Representations for Continual Learning
[] Channel Gating Neural Networks
[] XNAS: Neural Architecture Search with Expert Advice
[] Real-Time Reinforcement Learning
[] Training Language GANs from Scratch
[] Fast Efficient Hyperparameter Tuning for Policy Gradient Methods
[] Learning to Predict Without Looking Ahead: World Models Without Forward Prediction
[] XLNet: Generalized Autoregressive Pretraining for Language Understanding
[] Neural Machine Translation with Soft Prototype
[] Single-Model Uncertainties for Deep Learning
[] Large Scale Structure of Neural Network Loss Landscapes
[] Competitive Gradient Descent
[] Online Normalization for Training Neural Networks
[] Better Transfer Learning Through Inferred Successor Maps
[] Lookahead Optimizer: k steps forward, 1 step back
[] A Benchmark for Interpretability Methods in Deep Neural Networks
[] Learning to Learn By Self-Critique
[] Levenshtein Transformer
[] Meta Architecture Search
[] MetaInit: Initializing learning by learning to initialize
[] Episodic Memory in Lifelong Language Learning
[] Metalearned Neural Memory
[] Inducing brain-relevant bias in natural language processing models
[] Reinforcement Learning with Convex Constraints
[] Deep Leakage from Gradients
Self-attention with Functional Time Representation Learning
